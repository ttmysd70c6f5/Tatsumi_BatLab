{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de7119c-d9ec-4deb-b522-00d4c37bd40a",
   "metadata": {},
   "source": [
    "# Convert SpikeGadgets exported date into binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23db510-1a18-4959-9fc7-88a60bf2448b",
   "metadata": {},
   "source": [
    "## Recording directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e129659-1b9f-4a85-ade5-9eccf881bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of recording directory\n",
    "# rec_dir = 'C:/Users/Tatsumi/Documents/Data/KQTY_NP/32623/20230907_1min'\n",
    "# rec_dir = 'C:/Users/Tatsumi/Documents/Data/KQTY_NP/32623/20230907_161730.rec'\n",
    "rec_dir = 'E:/Dataset/KQTY/GridBat/32622/flight_room/231006/ephys/20231006_152611.rec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de6a57-9281-4594-ae5a-92f820f8a168",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e2b53f-a10f-4da4-9577-b0ad5470a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6382123-3fc2-4cde-8183-623339b4d866",
   "metadata": {},
   "source": [
    "The original trodes function `readTrodesExtractedDataFile3` cannot be imported because of the error caused by the testing function in the script. This is because the new version of NumPy does not support some syntax any more.  \n",
    "To load the data exported by Trodes, import and run the function copied and fixed for the part causing the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607d124f-20d7-4cc5-9cef-ae26c93c836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trodes function for data interface\n",
    "trodes_dir = 'C:/Users/Tatsumi/Documents/GitHub/Neuropixels_spikegadgets_analysis/python/fromTrodes/' # path of Trodes python functions\n",
    "sys.path.append(trodes_dir)\n",
    "from readTrodesExtractedDataFile3 import readTrodesExtractedDataFile as readTrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5a34f-00a9-4cfd-afef-760f03bfdd1b",
   "metadata": {},
   "source": [
    "First of all, specify the directories for each data band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62712f94-34e0-430c-a406-985f96800018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Dataset/KQTY/GridBat/32622/flight_room/231006/ephys/20231006_152611.rec/20231006_152611_merged.binary\n"
     ]
    }
   ],
   "source": [
    "lfp_dir = glob.glob('{}/*.LFP'.format(rec_dir))\n",
    "ap_dir = glob.glob('{}/*.spikeband'.format(rec_dir))\n",
    "if len(lfp_dir) == 1:\n",
    "    lfp_dir = Path(lfp_dir[0])\n",
    "if len(ap_dir) == 1:\n",
    "    ap_dir = Path(ap_dir[0])\n",
    "\n",
    "# Make directory for binary data\n",
    "file_header = lfp_dir.parts[-1].split('.')[0]\n",
    "binary_dir = rec_dir + '/' + file_header + '.binary'\n",
    "if not os.path.exists(binary_dir):\n",
    "    os.mkdir(binary_dir)\n",
    "print(binary_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a55aa-5ea6-4f16-ac15-c2c5bffb46da",
   "metadata": {},
   "source": [
    "Load the signals and timestamps for individual probe. Then, concatenate all channels into a binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51756142-e5d2-42e9-9809-9a7471df492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = ['LFP', 'spikeband']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e3c47-716b-4cd5-ab48-6748da9df81a",
   "metadata": {},
   "source": [
    "### Memmap to load long recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dac60b2-069d-4cc6-971f-36d1b01be066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21308\n",
      "24948\n",
      "29810\n",
      "29472\n",
      "29797\n",
      "26996\n",
      "26478\n",
      "15987\n",
      "2573\n",
      "25924\n"
     ]
    }
   ],
   "source": [
    "# fname = 'C:/Users/Tatsumi/Documents/Data/KQTY_NP/32623/20230907_1min/20230907_161730_merged_split1.binary/20230907_161730_merged_split1.LFP_mmap_probe2.dat'\n",
    "fname = 'E:/Dataset/KQTY/GridBat/32622/flight_room/231006/ephys/20231006_152611.rec/20231006_152611_merged.LFP/20231006_152611_merged.LFP_nt1161ch1.dat'\n",
    "f = open(fname,mode='rb')\n",
    "for i in range(10):\n",
    "    data = f.read(2)\n",
    "    print(int.from_bytes(data, \"little\"))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7101ab-6501-4711-bfe7-5bd704082ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = 'spikeband'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b691d-fb37-40c2-976c-10edcfe7f29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the stream spikeband...\n",
      "Probe 1 has 192 channels\n",
      "Probe 2 has 192 channels\n",
      "Probe 3 has 0 channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/Tatsumi/Documents/GitHub/Neuropixels_spikegadgets_analysis/python/fromTrodes\\readTrodesExtractedDataFile3.py:65: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return np.dtype(typearr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps has 195711745 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|███████████████████████████████████████████████████████████████| 192/192 [42:27<00:00, 13.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -384   320  -128 ...   512   640  1024]\n",
      " [ -192   128 -1024 ...  -512   192  -832]\n",
      " [ -320 -1088  -960 ...   128  -320 -1152]\n",
      " ...\n",
      " [-1408   192  -192 ...  2304  1984  2304]\n",
      " [  256 -1024 -1920 ...  2624  1920  2816]\n",
      " [-1088 -1472 -1984 ...  4736  3904  3840]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if stream == 'LFP':\n",
    "    stream_dir = lfp_dir\n",
    "if stream == 'spikeband':\n",
    "    stream_dir = ap_dir\n",
    "print('Processing the stream {}...'.format(stream))\n",
    "\n",
    "# gain = 50\n",
    "# scale_to_uv = (600/32767)*(1000/gain)\n",
    "\n",
    "num_chans = np.zeros(3) # Number of channels\n",
    "for probe in range(3):\n",
    "    # num_chans[probe] = int(len(glob.glob(stream_dir+'/*nt'+str(probe+1)+'*')))\n",
    "    num_chans[probe] = int(len(glob.glob('{}/*nt{}*'.format(stream_dir,probe+1))))\n",
    "    print('Probe {} has {} channels'.format(probe+1,int(num_chans[probe])))\n",
    "num_probe = sum(i > 0 for i in num_chans) # number of probes\n",
    "\n",
    "# Make a list of all dat files\n",
    "stream_file = {}\n",
    "for probe in range(num_probe):\n",
    "    stream_file[probe] = {}\n",
    "    stream_file[probe]['file_path'] = glob.glob('{}/*nt{}*'.format(stream_dir,probe+1))\n",
    "\n",
    "# Load timestamps\n",
    "ts_file = glob.glob('{}/*timestamps*'.format(stream_dir))\n",
    "ts_data = readTrodes(ts_file[0])\n",
    "ts_lfp = ts_data['data']\n",
    "print('Timestamps has {} samples'.format(ts_lfp.size))\n",
    "\n",
    "# concatenate data and write into a binary file\n",
    "for probe in range(num_probe):\n",
    "    binary_mmap_filename = os.path.join(binary_dir,'{}.{}_mmap_probe{}.dat'.format(file_header,stream,probe+1))\n",
    "    f = np.memmap(binary_mmap_filename, dtype='int16', mode='w+', shape=(int(num_chans[probe]),ts_lfp.size))\n",
    "    # f = open(binary_path,'wb')\n",
    "    # concatenated_binary = np.zeros([int(num_chans[probe]),ts_lfp.size],np.int16)\n",
    "    # for ch in range(int(num_chans[probe]))\n",
    "    for ch in tqdm(range(int(num_chans[probe])), desc=\"Loading data...\"):\n",
    "        # concatenated_binary[ch,:] = readTrodes(stream_file[probe]['file_path'][ch])['data']\n",
    "        f[ch,:] = readTrodes(stream_file[probe]['file_path'][ch])['data']\n",
    "    # concatenated_binary = concatenated_binary.reshape((concatenated_binary.size,1))\n",
    "\n",
    "    print(f)\n",
    "    f = f.transpose()\n",
    "    # f = f.reshape((1,f.size))\n",
    "    f.shape = (1,f.size)\n",
    "    print(f.shape)\n",
    "    print(f)\n",
    "    # f.astype('int16').tofile(binary_mmap_filename)\n",
    "    # f.write(concatenated_binary)\n",
    "    # f.close()\n",
    "    # f.flush()\n",
    "    del f\n",
    "    \n",
    "    print('-----------------------')\n",
    "    print('The stream {} for probe {} was converted into a binary file.'.format(stream,probe+1))\n",
    "    print('The path of the binary file is {}'.format(binary_mmap_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f13e6-8100-4886-8785-bba4a4841f42",
   "metadata": {},
   "source": [
    "### Concatenate without memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6eb9c150-489e-4611-ba78-85f9b5824c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the stream LFP...\n",
      "Probe 1 has 200 channels\n",
      "Probe 2 has 200 channels\n",
      "Probe 3 has 0 channels\n",
      "Timestamps has 6436518 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|███████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 34.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "The stream LFP for probe 1 was converted into a binary file.\n",
      "The path of the binary file is C:/Users/Tatsumi/Documents/Data/KQTY_NP/32623/20230907_161730.rec/20230907_161730_merged.binary\\20230907_161730_merged.LFP_probe1.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|███████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 33.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "The stream LFP for probe 2 was converted into a binary file.\n",
      "The path of the binary file is C:/Users/Tatsumi/Documents/Data/KQTY_NP/32623/20230907_161730.rec/20230907_161730_merged.binary\\20230907_161730_merged.LFP_probe2.dat\n",
      "\n",
      "\n",
      "Processing the stream spikeband...\n",
      "Probe 1 has 200 channels\n",
      "Probe 2 has 200 channels\n",
      "Probe 3 has 0 channels\n",
      "Timestamps has 128730794 samples\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 48.0 GiB for an array with shape (200, 128730794) and data type int16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m binary_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(binary_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_probe\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.dat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file_header,stream,probe\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     35\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(binary_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m concatenated_binary \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_chans\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mts_lfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# for ch in range(int(num_chans[probe])):\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(num_chans[probe])), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 48.0 GiB for an array with shape (200, 128730794) and data type int16"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for stream in streams:\n",
    "#     if stream == 'LFP':\n",
    "#         stream_dir = lfp_dir\n",
    "#     if stream == 'spikeband':\n",
    "#         stream_dir = ap_dir\n",
    "#     print('Processing the stream {}...'.format(stream))\n",
    "    \n",
    "#     # gain = 50\n",
    "#     # scale_to_uv = (600/32767)*(1000/gain)\n",
    "    \n",
    "#     num_chans = np.zeros(3) # Number of channels\n",
    "#     for probe in range(3):\n",
    "#         # num_chans[probe] = int(len(glob.glob(stream_dir+'/*nt'+str(probe+1)+'*')))\n",
    "#         num_chans[probe] = int(len(glob.glob('{}/*nt{}*'.format(stream_dir,probe+1))))\n",
    "#         print('Probe {} has {} channels'.format(probe+1,int(num_chans[probe])))\n",
    "#     num_probe = sum(i > 0 for i in num_chans) # number of probes\n",
    "    \n",
    "#     # Make a list of all dat files\n",
    "#     stream_file = {}\n",
    "#     for probe in range(num_probe):\n",
    "#         stream_file[probe] = {}\n",
    "#         stream_file[probe]['file_path'] = glob.glob('{}/*nt{}*'.format(stream_dir,probe+1))\n",
    "    \n",
    "#     # Load timestamps\n",
    "#     ts_file = glob.glob('{}/*timestamps*'.format(stream_dir))\n",
    "#     ts_data = readTrodes(ts_file[0])\n",
    "#     ts_lfp = ts_data['data']\n",
    "#     print('Timestamps has {} samples'.format(ts_lfp.size))\n",
    "    \n",
    "#     # concatenate data and write into a binary file\n",
    "#     for probe in range(num_probe):\n",
    "#         binary_path = os.path.join(binary_dir,'{}.{}_probe{}.dat'.format(file_header,stream,probe+1))\n",
    "#         f = open(binary_path,'wb')\n",
    "#         concatenated_binary = np.zeros([int(num_chans[probe]),ts_lfp.size],np.int16)\n",
    "#         # for ch in range(int(num_chans[probe])):\n",
    "#         for ch in tqdm(range(int(num_chans[probe])), desc=\"Loading data...\"):\n",
    "#             concatenated_binary[ch,:] = readTrodes(stream_file[probe]['file_path'][ch])['data']\n",
    "#         concatenated_binary = concatenated_binary.reshape((concatenated_binary.size,1))\n",
    "#         f.write(concatenated_binary)\n",
    "#         f.close()\n",
    "#         print('-----------------------')\n",
    "#         print('The stream {} for probe {} was converted into a binary file.'.format(stream,probe+1))\n",
    "#         print('The path of the binary file is {}'.format(binary_path))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a35aff-9dc4-4882-a4ca-427f72941fe1",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb90a245-4af9-46f2-b28a-85d430442305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe 1 has 192 channels\n",
      "Probe 2 has 192 channels\n",
      "Probe 3 has 0 channels\n",
      "Timestamps has 9785570 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/Tatsumi/Documents/GitHub/Neuropixels_spikegadgets_analysis/python/fromTrodes\\readTrodesExtractedDataFile3.py:65: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return np.dtype(typearr)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "stream = 'LFP'\n",
    "stream_dir = 'E:/Dataset/KQTY/GridBat/32622/flight_room/231006/ephys/20231006_152611.rec/20231006_152611_merged.LFP'\n",
    "\n",
    "num_chans = np.zeros(3) # Number of channels\n",
    "for probe in range(3):\n",
    "    # num_chans[probe] = int(len(glob.glob(stream_dir+'/*nt'+str(probe+1)+'*')))\n",
    "    num_chans[probe] = int(len(glob.glob('{}/*nt{}*'.format(stream_dir,probe+1))))\n",
    "    print('Probe {} has {} channels'.format(probe+1,int(num_chans[probe])))\n",
    "num_probe = sum(i > 0 for i in num_chans) # number of probes\n",
    "    \n",
    "# Make a list of all dat files\n",
    "stream_file = {}\n",
    "for probe in range(num_probe):\n",
    "    stream_file[probe] = {}\n",
    "    stream_file[probe]['file_path'] = glob.glob('{}/*nt{}*'.format(stream_dir,probe+1))\n",
    "\n",
    "# Load timestamps\n",
    "ts_file = glob.glob('{}/*timestamps*'.format(stream_dir))\n",
    "ts_data = readTrodes(ts_file[0])\n",
    "ts_lfp = ts_data['data']\n",
    "print('Timestamps has {} samples'.format(ts_lfp.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d69154-4f90-4d48-ad36-cfed42d0cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...: 100%|███████████████████████████████████████████████████████████████| 192/192 [00:03<00:00, 51.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -320  -192  -256 ... -1216 -1152 -1216]\n",
      " [    0    64  -128 ...     0   128   320]\n",
      " [  960  1024  1024 ... -1408 -1024 -1088]\n",
      " ...\n",
      " [ 1536  1408  1088 ...  3264  3584  3264]\n",
      " [  448   512   576 ... -1216 -1472 -1536]\n",
      " [  192   128   192 ... -1344 -1600 -1792]]\n",
      "(1, 1878829440)\n",
      "[[ -320     0   960 ...  3264 -1536 -1792]]\n"
     ]
    }
   ],
   "source": [
    "# concatenate data and write into a binary file\n",
    "probe = 0\n",
    "binary_mmap_filename = os.path.join(binary_dir,'{}.{}_mmap_probe{}.dat'.format(file_header,stream,probe+1))\n",
    "f = np.memmap(binary_mmap_filename, dtype='int16', mode='w+', shape=(int(num_chans[probe]),ts_lfp.size))\n",
    "# f = open(binary_path,'wb')\n",
    "# concatenated_binary = np.zeros([int(num_chans[probe]),ts_lfp.size],np.int16)\n",
    "# for ch in range(int(num_chans[probe]))\n",
    "for ch in tqdm(range(int(num_chans[probe])), desc=\"Loading data...\"):\n",
    "    # concatenated_binary[ch,:] = readTrodes(stream_file[probe]['file_path'][ch])['data']\n",
    "    f[ch,:] = readTrodes(stream_file[probe]['file_path'][ch])['data']\n",
    "# concatenated_binary = concatenated_binary.reshape((concatenated_binary.size,1))\n",
    "\n",
    "print(f)\n",
    "f = f.transpose()\n",
    "f = f.reshape((1,f.size))        \n",
    "print(f.shape)\n",
    "print(f)\n",
    "f.tofile(binary_mmap_filename)\n",
    "# f.write(concatenated_binary)\n",
    "# f.close()\n",
    "# f.flush()\n",
    "del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ee9592-024d-4417-9314-750f028daa00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m f\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b585349-cb0b-44c3-990b-160b19f05632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'C:/Users/Tatsumi/Documents/Data/KQTY_NP/32623/20230907_1min/20230907_161730_merged_split1.binary/20230907_161730_merged_split1.LFP_mmap_probe2.dat'\n",
    "fname = 'E:/Dataset/KQTY/GridBat/32622/flight_room/231006/ephys/20231006_152611.rec/20231006_152611_merged.binary/20231006_152611_merged.LFP_mmap_probe1.dat'\n",
    "f = np.memmap(fname,dtype='int16',mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07681c0a-477a-482c-85e4-513b68a036f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([ -320,     0,   960, ...,  3264, -1536, -1792], dtype=int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
